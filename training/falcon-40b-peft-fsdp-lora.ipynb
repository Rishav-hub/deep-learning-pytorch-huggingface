{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Falcon 40B using PEFT and LoRA with FSDP\n",
    "\n",
    "In this blog, we are going to learn how to to fine-tune [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) using [PEFT](https://github.com/huggingface/peft) with [Low-Rank Adaptation of Large Language Models (LoRA)](https://arxiv.org/abs/2106.09685) and PyTorch [FSDP](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/). PyTorch FSDP is natively integrated into the [Hugging Face Trainer](https://huggingface.co/docs/transformers/main_classes/trainer#pytorch-fully-sharded-data-parallel), making it easy to parallelize models on multiple GPUs. We are going to instruct-fine-tune Falcon-40B using the new [SFTTrainer](https://huggingface.co/docs/trl/main/en/sft_trainer) from the [trl](https://github.com/lvwerra/trl) library\n",
    "\n",
    "We will learn how to:\n",
    "1. Setup Development Environment and prepare the dataset\n",
    "2. Fine-Tune Falcon-40B with LoRA and FSDP\n",
    "3. Test Model and run Inference\n",
    "\n",
    "\n",
    "### Quick intro: PEFT or Parameter Efficient Fine-tuning\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of LLMs to various downstream applications without fine-tuning all the model's parameters. PEFT currently includes techniques for:\n",
    "\n",
    "- LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuning: [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuning: [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "\n",
    "*Note: This tutorial was created and run on a g5.48xlarge AWS EC2 Instance, including 1 NVIDIA A10G.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment and prepare the dataset\n",
    "\n",
    "In our example, we use the [PyTorch Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-pytorch.html) with already set up CUDA drivers and PyTorch installed. We still have to install the Hugging Face Libraries, including transformers and datasets. Running the following cell will install all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/conda/envs/pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/conda/envs/pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/huggingface/peft.git@189a6b8e357ecda05ccde13999e4c35759596a67\n",
      "  Cloning https://github.com/huggingface/peft.git (to revision 189a6b8e357ecda05ccde13999e4c35759596a67) to /tmp/pip-req-build-5ma1nivl\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-5ma1nivl\n",
      "  Running command git rev-parse -q --verify 'sha^189a6b8e357ecda05ccde13999e4c35759596a67'\n",
      "  Running command git fetch -q https://github.com/huggingface/peft.git 189a6b8e357ecda05ccde13999e4c35759596a67\n",
      "  Resolved https://github.com/huggingface/peft.git to commit 189a6b8e357ecda05ccde13999e4c35759596a67\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: safetensors in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from peft==0.4.0.dev0) (0.3.1)\n",
      "Requirement already satisfied: accelerate in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from peft==0.4.0.dev0) (0.20.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from peft==0.4.0.dev0) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from peft==0.4.0.dev0) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from peft==0.4.0.dev0) (1.23.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from peft==0.4.0.dev0) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from peft==0.4.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from peft==0.4.0.dev0) (4.30.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (11.7.101)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (10.2.10.91)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.6.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (11.7.99)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (10.9.0.58)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (11.7.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (2.14.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.4.0.dev0) (67.5.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.4.0.dev0) (0.38.4)\n",
      "Requirement already satisfied: cmake in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0.dev0) (3.26.0)\n",
      "Requirement already satisfied: lit in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0.dev0) (15.0.7)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers->peft==0.4.0.dev0) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers->peft==0.4.0.dev0) (0.14.1)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers->peft==0.4.0.dev0) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers->peft==0.4.0.dev0) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers->peft==0.4.0.dev0) (4.63.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0.dev0) (2023.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0.dev0) (2.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0.dev0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0.dev0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0.dev0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0.dev0) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from sympy->torch>=1.13.0->peft==0.4.0.dev0) (1.3.0)\n",
      "Building wheels for collected packages: peft\n",
      "  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peft: filename=peft-0.4.0.dev0-py3-none-any.whl size=59308 sha256=6d8f30bc8a77530201270c9056bcaf786dde4185b7a438e6b479bc78dae0aab2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-z89okwu5/wheels/0a/22/6b/f592af4574c71dd0d89c33777a404800f286d3cbd4daf7a22c\n",
      "Successfully built peft\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/conda/envs/pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ackaging (/opt/conda/envs/pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: peft 0.3.0\n",
      "    Uninstalling peft-0.3.0:\n",
      "      Successfully uninstalled peft-0.3.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/conda/envs/pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed peft-0.4.0.dev0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/conda/envs/pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/conda/envs/pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/conda/envs/pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"git+https://github.com/huggingface/peft.git@189a6b8e357ecda05ccde13999e4c35759596a67\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Hugging Face Libraries\n",
    "# !pip install \"peft==0.3.0\" \"trl==0.4.4\" \"transformers==4.30.1\" \"datasets==2.12.0\" \"accelerate==0.20.3\" \"evaluate==0.4.0\" loralib  --upgrade --quiet\n",
    "!pip install  \"trl==0.4.4\" \"transformers==4.30.1\" \"datasets==2.12.0\" \"accelerate==0.20.3\" \"evaluate==0.4.0\" loralib  --upgrade --quiet\n",
    "!pip install \"git+https://github.com/huggingface/peft.git@189a6b8e357ecda05ccde13999e4c35759596a67\"\n",
    "# install additional dependencies needed for training\n",
    "!pip install tensorboard \"torch==2.0.1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use the [dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k) an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the [InstructGPT paper](https://arxiv.org/abs/2203.02155), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"What is world of warcraft\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n",
    "}\n",
    "```\n",
    "\n",
    "> Note: The next steps are for demonstration. The dataset processing, formatting and tokenization will be part of the training script, [run_clm_fsdp_lora.py](./scripts/run_clm_fsdp_lora.py). \n",
    "\n",
    "To load the `databricks/databricks-dolly-15k` dataset, we use the `load_dataset()` method from the 🤗 Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# Train dataset size: 14732\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. Here is where the `SFTTrainer` from `trl` comes handy. The `SFTTrainer` supports formatting during training. This means we only need to define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "  instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "  context = f\"### Context\\n{sample['context']}\" if len(sample['context']) > 0 else None\n",
    "  response = f\"### Answer\\n{sample['response']}\"\n",
    "  # join all the parts together\n",
    "  prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "  return prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "What are the ways to save money in gardening?\n",
      "\n",
      "### Answer\n",
      "1. Avoid buying potting mix by making your own potting soil\n",
      "2. Compost your food scraps to make your own soil\n",
      "3. Avoid buying seed germinating trays by using tofu trays and other recycled food trays to germinate seeds\n",
      "4. Avoid buying pots and containers by re-using plastic milk containers with the top cut off, tetra pak with the top cut off, yoghurt containers, plastic soda bottles etc.\n",
      "5. Avoid buying plants from the store by germinating plants from seed yourself\n",
      "6. Collect rainwater for your plants to avoid using municipal water\n",
      "7. Re-use water from rinsing vegetables/rice to water your plants to minimize the use of municipal water\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fine-Tune Falcon-40B with LoRA and FSDP\n",
    "\n",
    "We are going to use PyTorch FSDP to train Falcon-40B on multiple GPUs, this means we need to use a distributed launcher, e.g. `torchrun` to start our training on multiple-gpus. We are formatting and tokenizing the dataset during the training using the `formatting_func` function of the `SFTTrainer`. \n",
    "\n",
    "The `SFTTrainer` also supports a native integration with `peft`, which makes it super easy to efficiently instruction tune LLMs. We prepared a [run_clm_fsdp_lora.py](./scripts/run_clm_fsdp_lora.py), which implements causal language modeling and accepts all relevant parameters, including the model id, peft configuration. The `SFTTrainer` part in our scripts looks like this:\n",
    "\n",
    "```python\n",
    "trainer = SFTTrainer(\n",
    "    model, # our loaded model\n",
    "    args=training_args, # our training args\n",
    "    train_dataset=dataset, # raw training dataset\n",
    "    formatting_func=format_dolly, # formatting function\n",
    "    peft_config=peft_config, # peft config\n",
    "    packing=True, # wether to pack data samples to max length\n",
    "    max_seq_length=2048 # max sequence length for packing\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing#scrollTo=dQdvjTYTT1vQ\n",
    "https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "!python scripts/run_clm_fsdp_lora.py \\\n",
    " --model_id tiiuae/falcon-7b \\\n",
    " --dataset_id \"databricks/databricks-dolly-15k\" \\\n",
    " --per_device_train_batch_size 1 \\\n",
    " --num_train_epochs 1 \\\n",
    " --learning_rate 2e-4 \\\n",
    " --gradient_checkpointing True \\\n",
    " --bf16 True \\\n",
    " --tf32 True \\\n",
    " --output_dir ./tmp \\\n",
    " --logging_steps 10\n",
    " \n",
    " \n",
    " #--optim adamw_apex_fused \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_ID=\"tiiuae/falcon-40b\"\n",
    "DATASET_ID=\"databricks/databricks-dolly-15k\"\n",
    "NUM_GPUS=8\n",
    "\n",
    "echo \"Training ${MODEL_ID} on ${DATASET_ID} using ${NUM_GPUS} GPU.\n",
    "\n",
    "torchrun --nproc_per_node ${NUM_GPUS} scripts/run_clm_fsdp_lora.py \\\n",
    "  --model_id ${MODEL_ID} \\\n",
    "  --dataset_id ${DATASET_ID} \\\n",
    "  --per_device_train_batch_size 1 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --learning_rate 2e-4 \\\n",
    "  --gradient_checkpointing True \\\n",
    "  --bf16 True \\\n",
    "  --tf32 True \\\n",
    "  --output_dir ./tmp \\\n",
    "  --logging_steps 10 \\\n",
    "  --fsdp \"full_shard auto_wrap\" \\\n",
    "  --fsdp_transformer_layer_cls_to_wrap \"DecoderLayer\"\n",
    "  # --optim adamw_apex_fused \\"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training took ~10:36:00 and cost `~13.22$` for 10h of training. For comparison a [full fine-tuning on FLAN-T5-XXL](https://www.philschmid.de/fine-tune-flan-t5-deepspeed#3-results--experiments) with the same duration (10h) requires 8x A100 40GBs and costs ~322$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Model and run Inference\n",
    "\n",
    "After the training is done we want to run and test our model. We will use `peft` and `transformers` to load our LoRA adapter into our model. We will also use `accelerate` to run our inference on multiple GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load peft config for pre-trained checkpoint etc. \n",
    "peft_model_id = \"tmp\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "model.eval()\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s load the dataset again with a random sample to try the summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randrange\n",
    "\n",
    "\n",
    "# Load dataset from the hub and get a sample\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "sample = dataset[randrange(len(dataset))]\n",
    "\n",
    "prompt = f\"### Instruction\\n{sample['instruction']}\\n\\n\"\n",
    "if len(sample['context']) > 0:\n",
    "  prompt += f\"### Context\\n{sample['context']}\\n\\n\"\n",
    "prompt += f\"### Answer\\n\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=50, do_sample=True, top_p=0.9)\n",
    "\n",
    "print(f\"{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! our model works! If want to accelerate our model we can deploy it with [Text Generation Inference](https://github.com/huggingface/text-generation-inference). Therefore we would need to merge our adapter weights into the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA and base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resources:\n",
    "\n",
    "* https://huggingface.co/docs/transformers/peft#add-a-new-adapter\n",
    "* https://github.com/huggingface/trl/blob/2f726ce4e88a99b5d20eca3b5482954851d91ef6/trl/trainer/utils.py#L482\n",
    "* https://github.com/huggingface/tokenizers/issues/247\n",
    "* https://github.com/philschmid/llm-sagemaker-sample/blob/main/scripts/run_qlora.py\n",
    "* https://huggingface.co/OpenAssistant/codellama-13b-oasst-sft-v10/blob/main/special_tokens_map.json\n",
    "* https://github.com/huggingface/trl/blob/v0.7.4/trl/trainer/sft_trainer.py#L50\n",
    "* https://huggingface.co/docs/trl/v0.7.4/en/sft_trainer#packing-dataset--constantlengthdataset-\n",
    "* https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template\n",
    "* https://github.com/huggingface/trl/blob/2f726ce4e88a99b5d20eca3b5482954851d91ef6/trl/trainer/utils.py#L133\n",
    "* https://huggingface.co/docs/trl/sft_trainer\n",
    "* https://huggingface.co/docs/transformers/peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, PreTrainedTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "path_or_dataset_id = \"philschmid/guanaco-oai-style\"\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_token = '<|im_start|>assistant'\n",
    "user_token = '<|im_start|>user'\n",
    "system_token = '<|im_start|>system'\n",
    "    \n",
    "def create_tokenizer(tokenizer_id:str) -> PreTrainedTokenizer:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "    \n",
    "    # new special tokens\n",
    "    bos_token= '<|im_start|>'\n",
    "    eos_token= '<|im_end|>'\n",
    "\n",
    "    tokenizer.chat_template = (\n",
    "        \"{% for message in messages %}\"\n",
    "        \"{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}\"\n",
    "        \"{% endfor %}\"\n",
    "        \"{% if add_generation_prompt %}\"\n",
    "        \"{{ '<|im_start|>assistant\\n' }}\"\n",
    "        \"{% endif %}\"\n",
    "    )\n",
    "    # https://huggingface.co/OpenAssistant/codellama-13b-oasst-sft-v10/blob/main/special_tokens_map.json\n",
    "    tokenizer.eos_token = eos_token\n",
    "    tokenizer.pad_token = eos_token\n",
    "    tokenizer.bos_token = bos_token\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": [bos_token, eos_token]})\n",
    "    \n",
    "    return tokenizer\n",
    "    # model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "tokenizer = create_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvalidMessagesError(Exception):\n",
    "  pass\n",
    "\n",
    "def validate_messages(data):\n",
    "  if \"messages\" not in data:\n",
    "    raise InvalidMessagesError(\"No 'messages' key in data\")  \n",
    "\n",
    "  messages = data[\"messages\"]\n",
    "  if not isinstance(messages, list):\n",
    "    raise InvalidMessagesError(\"messages is not a list\")\n",
    "\n",
    "  if not all(isinstance(m, dict) and \"role\" in m and \"content\" in m for m in messages):\n",
    "    raise InvalidMessagesError(\"One or more invalid messages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/philschmid___json/philschmid--guanaco-oai-style-fafc168945666df2/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/philschmid___json/philschmid--guanaco-oai-style-fafc168945666df2/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-45809af9be7206da_*_of_00008.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "How to write docker yaml file<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To write a Docker YAML file, you will need to create a Docker Compose file which is a YAML file used to define and run multi-container Docker applications.\n",
      "\n",
      "Here's a basic example of a Docker Compose file:\n",
      "\n",
      "```yaml\n",
      "version: '3.9'\n",
      "services:\n",
      "  web:\n",
      "    build: .\n",
      "    ports:\n",
      "      - \"5000:5000\"\n",
      "  redis:\n",
      "    image: \"redis:alpine\"\n",
      "```\n",
      "\n",
      "In this example, there are two services: `web` and `redis`. The `web` service is built from the current directory (`.`) and exposes port 5000 on the host. The `redis` service uses the Redis Alpine image.\n",
      "\n",
      "Here's an explanation of the key components in this file:\n",
      "\n",
      "- `version`: specifies the version of the Docker Compose file format to use.\n",
      "- `services`: a list of services that make up your application.\n",
      "- `build`: specifies the path to the Dockerfile to use to build the container.\n",
      "- `image`: specifies the name of the image to use for the container.\n",
      "- `ports`: specifies the ports to expose on the host machine.\n",
      "\n",
      "You can add additional configuration options to each service as needed, such as environment variables, volumes, and more. Once you've created your Docker Compose file, you can use the `docker-compose` command to start and stop your application.<|im_end|>\n",
      "<|im_start|>user\n",
      "Optimize the code<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The code provided is a YAML file and is already quite optimized. However, here are a few suggestions to make the code even more efficient:\n",
      "\n",
      "1. Use the latest version of Docker Compose if possible to take advantage of the latest features and improvements.\n",
      "2. Use the shorthand notation for port mapping to make the code more concise. For example, \"5000:5000\" can be written as \"5000\".\n",
      "3. Avoid using unnecessary quotes around values unless they contain special characters. For example, \"redis:alpine\" can be written as redis:alpine.\n",
      "4. Use caching when building containers to speed up the build process. This can be done by adding a cache_from parameter to the build section.\n",
      "Here is an optimized version of the code with these suggestions applied:\n",
      "\n",
      "```\n",
      "version: '3.9'\n",
      "services:\n",
      "web:\n",
      "build: .\n",
      "ports:\n",
      "- 5000\n",
      "redis:\n",
      "image: redis:alpine\n",
      "cache_from:\n",
      "- redis:alpine\n",
      "```<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from random import randint\n",
    "from trl.trainer.utils import DataCollatorForCompletionOnlyLM, ConstantLengthDataset\n",
    "\n",
    "# Load raw dataset from the hub or disk\n",
    "raw_json = load_dataset(path_or_dataset_id,split=\"train\")\n",
    "\n",
    "# check if dataset has messages key and if elements are conversations with Dict[str, str] elements and with keys of role, content\n",
    "validate_messages(raw_json.features)\n",
    "\n",
    "# apply chat template and tokenize \n",
    "template_ds = raw_json.map(lambda s: {\"prompt\":tokenizer.apply_chat_template(s[\"messages\"], tokenize=False)},remove_columns=raw_json.features, num_proc=os.cpu_count())\n",
    "# print random example\n",
    "print(template_ds[randint(0,len(template_ds))][\"prompt\"])\n",
    "ds = ConstantLengthDataset(tokenizer, dataset=template_ds, seq_length=2048,dataset_text_field=\"prompt\")\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=user_token, response_template=assistant_token, tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006926774978637695,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a002b2b9089d4beb929531d417c8bcc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig,TaskType\n",
    "    \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        # use_cache=False if training_args.gradient_checkpointing else True,  # this is needed for gradient checkpointing\n",
    "        use_cache=False,  # this is needed for gradient checkpointing\n",
    "        device_map=\"auto\",\n",
    "        # use_flash_attention_2=script_args.use_flash_attn,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# COPIED FROM https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "def find_all_linear_names(model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, bnb.nn.Linear4bit):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model.add_adapter(peft_config, adapter_name=\"adapter_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "args = TrainingArguments(output_dir=\"test\",per_device_train_batch_size=1,per_device_eval_batch_size=1,gradient_checkpointing=True,logging_steps=10)\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=ds,\n",
    "    model=model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='89' max='27099' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   89/27099 05:40 < 29:22:30, 0.26 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.508200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.210600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.894100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.828400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.159300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.024800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/trl/trainer/utils.py:147: UserWarning: Could not find response key `<|im_start|>assistant` in the following instance: ле 70-х годов компьютеры начали понемногу приближаться к тем формам, которые знакомы нам сейчас. Именно тогда начали создаваться первые микрокомпьютеры. К 1971 году к сети ARPANet были подключены еще 15 терминалов для приема и передачи данных. В этом же году программист Реймонд Томлинсон впервые сделал возможным отправку по этой сети электронных писем. Он же придумал использовать для этого знаменитую «собачку» - символ «@». Кстати, точная причина того, почему он называется именно собачкой, неизвестна. При этом полноценной коммуникации мешал один существенный фактор, а именно различия в интерфейсе в разных сетях.\n",
      "\n",
      "К 1973 году стало возможным обмениваться сообщениями не только внутри американского континента, но и передавать информацию в Европу. Это было сделано благодаря трансатлантическому телефонному кабелю, первые из которых были проложены еще в 50-е годы. При этом никуда не делась упомянутая нами проблема с интерфейсом.\n",
      "\n",
      "Поэтому в 1976 году, теперь уже DARPANetначинает финансировать создание единого унифицированного протокола под названием ТСР/IP, которые на долгое время станет эталоном для функционирования глобальных сетей. В 1983 году сеть полностью перешла на его использование.\n",
      "\n",
      "В 1984 году начинается разработка системы DNS– Domain Name System. Каждое устройство, включенное в систему DARPANet, имело свой числовой адрес. С ростом числа устройств становилось все труднее сопоставлять буквенные адреса с числовыми, а делать это приходилось вручную. В этом же году был разработан первый сервер доменных имен, получивший свое название от университета, где обучались четыре студента, которые разработали BIND – BerkleyInternet Name Daemon. Он автоматически связывал буквенные наименования к числовым значениям. В начале 90-х этот сервер был использовать на операционной системе отWindows, конечно, в улучшенном и пересмотренном виде.\n",
      "\n",
      "В 1985 году у DARPANet появляется прямой конкурент, который впоследствии полностью его поглотит – NSFNET, компьютерная сеть национального фонда реализовывает программу по созданию сетей вокруг своих центров. Через год создается обширная сеть региональных сетей, которые впоследствии были связаны с DARPANet. Отдельные устройства, входящие в сеть, замыкались на суперкомпьютер в центре NSF. Это положило начало глобализации сети Интернет. К концу 80-х годов к сети подключились и другие страны – прежде всего Канада, Финляндия, Франция, Дания, Норвегия, Швеция.\n",
      "\n",
      "В 1988 году молодой интернет поразил один из первых сетевых вирусов, который носил название Червь Морриса. Он был способен к саморепликации, перезаписывая свою копию с определенной периодичностью. Его действие состояло в критическом замедлении работы компьютера – вплоть до полного отключения. Ущерб, который нанес вирус, оценивается приблизительно в 96,5 млн $. На суде Моррис, который оказался студентом Корнуэльского университета, заявил, что не имел никакого злого умысла, а просто хотел «узнать, насколько большой интернет».\n",
      "\n",
      "В 1990 году всемирная сеть насчитывала уже более 160 000 узлов связи. В это же время начинает набирать популярность технология передачи данных при помощи оптоволоконных кабелей. Принцип работы данного метода передачи информации состоит в передаче двоичного кода при помощи световых сигналов, распространяемых посредством кабеля из оптического волокна, стенки которого отражают свет. Однако на тот момент гораздо более распространенным был классический медный кабель. Это было связано с таким минусом оптоволокна как регулярное затухание, преодолеть которое удалось лишь спустя два десятилетия.\n",
      "\n",
      "В этом же году произошло первое подключение к интернету посредством мобильной сети. Устройство носило название Dial-UP, и оно позволило компьютеру посредством относительно компактного модема устанавливать сеанс связи для передачи каких-либо данных.\n",
      "\n",
      "В 1990 году в Советском Союзе появилась сеть, предоставляющая доступ ко Всемирной паутине. Она получила название GlasNet – первая часть представляет собой сокращение от слова «гласность». В большинстве крупных городов нашей страны начали появляться узлы связи, наращивали обороты первые провайдеры – такие как Релком, Совинтел.\n",
      "\n",
      "В 1993 году веб-страницы начинают приобретать знакомую нам структуру. Появляется HTML – язык гипертекстовой разметки. Он был разработан, на этот раз, не американцем, а британским программистом по имени Тим Бернерс-Ли. Это послужило толчком появлениюWorld Wide Web, сочетание первых трех букв которой составляет название любого интернет-сайта. Совокупность всех его достижений и позволила нам видеть интернет в том виде, в котором мы его знаем сейчас – а не просто как способ передачи данных между двумя точками. Основу работ Бернерса-Ли составили:\n",
      "\n",
      "HTML. Это язык гипертекстовой разметки веб-страницы, который позволяет определить, каким образом представляется загруженная страница.\n",
      "HTPP. Это механизм доступа к веб-страницам и навигации между ними, который выступает основой обмена информацией во Всемирной сети.\n",
      "URL. Специальный адресный код, который присваивается каждому гипертекстовому документу.\n",
      "\n",
      "Система WEB-сайтов полностью перевернула представление об интернете. Из простого способа коммуникации он превратился в глобальное и универсальное информационное пространство. В 1991 году все тем же Тимом Бернерсом-Ли был создам первый в истории веб-сайт. На этой страничке была опубликована информация о World Wide Web, а впоследствии стала первой базой ссылок на другие сайты.\n",
      "\n",
      "Опуская многие This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/trl/trainer/utils.py:162: UserWarning: Could not find instruction key `<|im_start|>user` in the following instance: ле 70-х годов компьютеры начали понемногу приближаться к тем формам, которые знакомы нам сейчас. Именно тогда начали создаваться первые микрокомпьютеры. К 1971 году к сети ARPANet были подключены еще 15 терминалов для приема и передачи данных. В этом же году программист Реймонд Томлинсон впервые сделал возможным отправку по этой сети электронных писем. Он же придумал использовать для этого знаменитую «собачку» - символ «@». Кстати, точная причина того, почему он называется именно собачкой, неизвестна. При этом полноценной коммуникации мешал один существенный фактор, а именно различия в интерфейсе в разных сетях.\n",
      "\n",
      "К 1973 году стало возможным обмениваться сообщениями не только внутри американского континента, но и передавать информацию в Европу. Это было сделано благодаря трансатлантическому телефонному кабелю, первые из которых были проложены еще в 50-е годы. При этом никуда не делась упомянутая нами проблема с интерфейсом.\n",
      "\n",
      "Поэтому в 1976 году, теперь уже DARPANetначинает финансировать создание единого унифицированного протокола под названием ТСР/IP, которые на долгое время станет эталоном для функционирования глобальных сетей. В 1983 году сеть полностью перешла на его использование.\n",
      "\n",
      "В 1984 году начинается разработка системы DNS– Domain Name System. Каждое устройство, включенное в систему DARPANet, имело свой числовой адрес. С ростом числа устройств становилось все труднее сопоставлять буквенные адреса с числовыми, а делать это приходилось вручную. В этом же году был разработан первый сервер доменных имен, получивший свое название от университета, где обучались четыре студента, которые разработали BIND – BerkleyInternet Name Daemon. Он автоматически связывал буквенные наименования к числовым значениям. В начале 90-х этот сервер был использовать на операционной системе отWindows, конечно, в улучшенном и пересмотренном виде.\n",
      "\n",
      "В 1985 году у DARPANet появляется прямой конкурент, который впоследствии полностью его поглотит – NSFNET, компьютерная сеть национального фонда реализовывает программу по созданию сетей вокруг своих центров. Через год создается обширная сеть региональных сетей, которые впоследствии были связаны с DARPANet. Отдельные устройства, входящие в сеть, замыкались на суперкомпьютер в центре NSF. Это положило начало глобализации сети Интернет. К концу 80-х годов к сети подключились и другие страны – прежде всего Канада, Финляндия, Франция, Дания, Норвегия, Швеция.\n",
      "\n",
      "В 1988 году молодой интернет поразил один из первых сетевых вирусов, который носил название Червь Морриса. Он был способен к саморепликации, перезаписывая свою копию с определенной периодичностью. Его действие состояло в критическом замедлении работы компьютера – вплоть до полного отключения. Ущерб, который нанес вирус, оценивается приблизительно в 96,5 млн $. На суде Моррис, который оказался студентом Корнуэльского университета, заявил, что не имел никакого злого умысла, а просто хотел «узнать, насколько большой интернет».\n",
      "\n",
      "В 1990 году всемирная сеть насчитывала уже более 160 000 узлов связи. В это же время начинает набирать популярность технология передачи данных при помощи оптоволоконных кабелей. Принцип работы данного метода передачи информации состоит в передаче двоичного кода при помощи световых сигналов, распространяемых посредством кабеля из оптического волокна, стенки которого отражают свет. Однако на тот момент гораздо более распространенным был классический медный кабель. Это было связано с таким минусом оптоволокна как регулярное затухание, преодолеть которое удалось лишь спустя два десятилетия.\n",
      "\n",
      "В этом же году произошло первое подключение к интернету посредством мобильной сети. Устройство носило название Dial-UP, и оно позволило компьютеру посредством относительно компактного модема устанавливать сеанс связи для передачи каких-либо данных.\n",
      "\n",
      "В 1990 году в Советском Союзе появилась сеть, предоставляющая доступ ко Всемирной паутине. Она получила название GlasNet – первая часть представляет собой сокращение от слова «гласность». В большинстве крупных городов нашей страны начали появляться узлы связи, наращивали обороты первые провайдеры – такие как Релком, Совинтел.\n",
      "\n",
      "В 1993 году веб-страницы начинают приобретать знакомую нам структуру. Появляется HTML – язык гипертекстовой разметки. Он был разработан, на этот раз, не американцем, а британским программистом по имени Тим Бернерс-Ли. Это послужило толчком появлениюWorld Wide Web, сочетание первых трех букв которой составляет название любого интернет-сайта. Совокупность всех его достижений и позволила нам видеть интернет в том виде, в котором мы его знаем сейчас – а не просто как способ передачи данных между двумя точками. Основу работ Бернерса-Ли составили:\n",
      "\n",
      "HTML. Это язык гипертекстовой разметки веб-страницы, который позволяет определить, каким образом представляется загруженная страница.\n",
      "HTPP. Это механизм доступа к веб-страницам и навигации между ними, который выступает основой обмена информацией во Всемирной сети.\n",
      "URL. Специальный адресный код, который присваивается каждому гипертекстовому документу.\n",
      "\n",
      "Система WEB-сайтов полностью перевернула представление об интернете. Из простого способа коммуникации он превратился в глобальное и универсальное информационное пространство. В 1991 году все тем же Тимом Бернерсом-Ли был создам первый в истории веб-сайт. На этой страничке была опубликована информация о World Wide Web, а впоследствии стала первой базой ссылок на другие сайты.\n",
      "\n",
      "Опуская многие This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/deep-learning-pytorch-huggingface/dev.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bg5/home/ubuntu/deep-learning-pytorch-huggingface/dev.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1862\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/trainer.py:2734\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2733\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2734\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m   2736\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/accelerate/accelerator.py:1923\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1921\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1922\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1923\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
